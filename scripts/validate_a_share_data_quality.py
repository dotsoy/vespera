#!/usr/bin/env python3
"""
Aè‚¡æ•°æ®è´¨é‡éªŒè¯è„šæœ¬
éªŒè¯å¯¼å…¥çš„Aè‚¡ç”Ÿäº§æ•°æ®çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
"""
import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import get_logger
from src.utils.stock_filter import StockFilter

logger = get_logger("validate_a_share_data_quality")


def validate_data_completeness(data: pd.DataFrame) -> dict:
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§")
    
    try:
        results = {
            'total_records': len(data),
            'unique_stocks': data['ts_code'].nunique(),
            'date_range': {
                'start': data['trade_date'].min(),
                'end': data['trade_date'].max()
            },
            'missing_values': {},
            'completeness_score': 0
        }
        
        # æ£€æŸ¥å…³é”®å­—æ®µçš„ç¼ºå¤±å€¼
        key_columns = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'vol', 'amount']
        
        for col in key_columns:
            if col in data.columns:
                missing_count = data[col].isnull().sum()
                missing_pct = (missing_count / len(data)) * 100
                results['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # è®¡ç®—å®Œæ•´æ€§è¯„åˆ†
        total_missing = sum(item['count'] for item in results['missing_values'].values())
        total_possible = len(data) * len(key_columns)
        results['completeness_score'] = ((total_possible - total_missing) / total_possible) * 100
        
        logger.success(f"âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
        logger.info(f"æ€»è®°å½•æ•°: {results['total_records']}")
        logger.info(f"è‚¡ç¥¨æ•°é‡: {results['unique_stocks']}")
        logger.info(f"æ—¥æœŸèŒƒå›´: {results['date_range']['start']} åˆ° {results['date_range']['end']}")
        logger.info(f"å®Œæ•´æ€§è¯„åˆ†: {results['completeness_score']:.2f}%")
        
        # æ˜¾ç¤ºç¼ºå¤±å€¼æƒ…å†µ
        for col, missing_info in results['missing_values'].items():
            if missing_info['count'] > 0:
                logger.warning(f"âš ï¸ {col}: ç¼ºå¤± {missing_info['count']} ä¸ªå€¼ ({missing_info['percentage']:.2f}%)")
            else:
                logger.info(f"âœ… {col}: æ— ç¼ºå¤±å€¼")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
        return {}


def validate_data_accuracy(data: pd.DataFrame) -> dict:
    """éªŒè¯æ•°æ®å‡†ç¡®æ€§"""
    logger.info("ğŸ¯ éªŒè¯æ•°æ®å‡†ç¡®æ€§")
    
    try:
        results = {
            'price_logic_errors': 0,
            'volume_anomalies': 0,
            'pct_change_errors': 0,
            'date_format_errors': 0,
            'accuracy_score': 0
        }
        
        total_errors = 0
        total_checks = 0
        
        # 1. ä»·æ ¼é€»è¾‘æ£€æŸ¥
        logger.info("æ£€æŸ¥ä»·æ ¼é€»è¾‘...")
        price_logic_mask = (
            (data['high'] >= data['open']) & 
            (data['high'] >= data['close']) & 
            (data['low'] <= data['open']) & 
            (data['low'] <= data['close']) &
            (data['high'] >= data['low'])
        )
        price_errors = (~price_logic_mask).sum()
        results['price_logic_errors'] = price_errors
        total_errors += price_errors
        total_checks += len(data)
        
        if price_errors > 0:
            logger.warning(f"âš ï¸ å‘ç° {price_errors} æ¡ä»·æ ¼é€»è¾‘é”™è¯¯")
        else:
            logger.success("âœ… ä»·æ ¼é€»è¾‘æ£€æŸ¥é€šè¿‡")
        
        # 2. æˆäº¤é‡å¼‚å¸¸æ£€æŸ¥
        logger.info("æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸...")
        volume_anomalies = (data['vol'] < 0).sum()
        results['volume_anomalies'] = volume_anomalies
        total_errors += volume_anomalies
        total_checks += len(data)
        
        if volume_anomalies > 0:
            logger.warning(f"âš ï¸ å‘ç° {volume_anomalies} æ¡è´Ÿæˆäº¤é‡")
        else:
            logger.success("âœ… æˆäº¤é‡æ£€æŸ¥é€šè¿‡")
        
        # 3. æ¶¨è·Œå¹…è®¡ç®—æ£€æŸ¥
        logger.info("æ£€æŸ¥æ¶¨è·Œå¹…è®¡ç®—...")
        if 'pct_chg' in data.columns and 'close' in data.columns and 'pre_close' in data.columns:
            calculated_pct_chg = ((data['close'] - data['pre_close']) / data['pre_close'] * 100).round(2)
            pct_chg_errors = (abs(data['pct_chg'] - calculated_pct_chg) > 0.1).sum()
            results['pct_change_errors'] = pct_chg_errors
            total_errors += pct_chg_errors
            total_checks += len(data)
            
            if pct_chg_errors > 0:
                logger.warning(f"âš ï¸ å‘ç° {pct_chg_errors} æ¡æ¶¨è·Œå¹…è®¡ç®—é”™è¯¯")
            else:
                logger.success("âœ… æ¶¨è·Œå¹…è®¡ç®—æ£€æŸ¥é€šè¿‡")
        
        # 4. æ—¥æœŸæ ¼å¼æ£€æŸ¥
        logger.info("æ£€æŸ¥æ—¥æœŸæ ¼å¼...")
        try:
            pd.to_datetime(data['trade_date'])
            logger.success("âœ… æ—¥æœŸæ ¼å¼æ£€æŸ¥é€šè¿‡")
        except:
            results['date_format_errors'] = len(data)
            total_errors += len(data)
            total_checks += len(data)
            logger.error("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
        
        # è®¡ç®—å‡†ç¡®æ€§è¯„åˆ†
        if total_checks > 0:
            results['accuracy_score'] = ((total_checks - total_errors) / total_checks) * 100
        
        logger.success(f"âœ… æ•°æ®å‡†ç¡®æ€§éªŒè¯å®Œæˆ")
        logger.info(f"å‡†ç¡®æ€§è¯„åˆ†: {results['accuracy_score']:.2f}%")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å‡†ç¡®æ€§éªŒè¯å¤±è´¥: {e}")
        return {}


def validate_a_share_characteristics(data: pd.DataFrame) -> dict:
    """éªŒè¯Aè‚¡å¸‚åœºç‰¹å¾"""
    logger.info("ğŸ›ï¸ éªŒè¯Aè‚¡å¸‚åœºç‰¹å¾")
    
    try:
        results = {
            'limit_violations': 0,
            'trading_day_consistency': True,
            'market_distribution': {},
            'industry_coverage': {},
            'a_share_compliance_score': 0
        }
        
        # 1. æ¶¨è·Œåœé™åˆ¶æ£€æŸ¥
        logger.info("æ£€æŸ¥æ¶¨è·Œåœé™åˆ¶...")
        if 'pct_chg' in data.columns:
            # æ™®é€šè‚¡ç¥¨Â±10%ï¼Œç§‘åˆ›æ¿åˆ›ä¸šæ¿Â±20%
            gem_stocks = data['ts_code'].str.startswith(('300', '688'))
            
            # æ™®é€šè‚¡ç¥¨æ¶¨è·Œåœæ£€æŸ¥
            normal_stocks = ~gem_stocks
            normal_violations = (
                (data[normal_stocks]['pct_chg'] > 10.1) | 
                (data[normal_stocks]['pct_chg'] < -10.1)
            ).sum()
            
            # ç§‘åˆ›æ¿åˆ›ä¸šæ¿æ¶¨è·Œåœæ£€æŸ¥
            gem_violations = (
                (data[gem_stocks]['pct_chg'] > 20.1) | 
                (data[gem_stocks]['pct_chg'] < -20.1)
            ).sum()
            
            results['limit_violations'] = normal_violations + gem_violations
            
            if results['limit_violations'] > 0:
                logger.warning(f"âš ï¸ å‘ç° {results['limit_violations']} æ¡æ¶¨è·Œåœè¿è§„")
            else:
                logger.success("âœ… æ¶¨è·Œåœé™åˆ¶æ£€æŸ¥é€šè¿‡")
        
        # 2. å¸‚åœºåˆ†å¸ƒæ£€æŸ¥
        logger.info("æ£€æŸ¥å¸‚åœºåˆ†å¸ƒ...")
        if 'ts_code' in data.columns:
            sz_count = data['ts_code'].str.endswith('.SZ').sum()
            sh_count = data['ts_code'].str.endswith('.SH').sum()
            other_count = len(data) - sz_count - sh_count
            
            results['market_distribution'] = {
                'SZ': sz_count,
                'SH': sh_count,
                'Other': other_count
            }
            
            logger.info(f"å¸‚åœºåˆ†å¸ƒ: æ·±äº¤æ‰€ {sz_count}, ä¸Šäº¤æ‰€ {sh_count}, å…¶ä»– {other_count}")
            
            if other_count > 0:
                logger.warning(f"âš ï¸ å‘ç° {other_count} æ¡éæ²ªæ·±è‚¡ç¥¨æ•°æ®")
        
        # 3. è¡Œä¸šè¦†ç›–æ£€æŸ¥
        logger.info("æ£€æŸ¥è¡Œä¸šè¦†ç›–...")
        if 'industry' in data.columns:
            industry_counts = data['industry'].value_counts()
            results['industry_coverage'] = industry_counts.to_dict()
            
            logger.info(f"è¡Œä¸šè¦†ç›–: {len(industry_counts)} ä¸ªè¡Œä¸š")
            for industry, count in industry_counts.head().items():
                logger.info(f"  {industry}: {count} æ¡è®°å½•")
        
        # 4. è‚¡ç¥¨ä»£ç æ ¼å¼æ£€æŸ¥
        logger.info("æ£€æŸ¥è‚¡ç¥¨ä»£ç æ ¼å¼...")
        stock_filter = StockFilter()
        valid_codes = 0
        
        for code in data['ts_code'].unique():
            validation = stock_filter.validate_stock_code(code)
            if validation['is_a_share']:
                valid_codes += 1
        
        code_compliance = (valid_codes / data['ts_code'].nunique()) * 100
        
        # è®¡ç®—Aè‚¡åˆè§„æ€§è¯„åˆ†
        compliance_factors = [
            (results['limit_violations'] == 0, 30),  # æ¶¨è·Œåœåˆè§„
            (results['market_distribution']['Other'] == 0, 25),  # å¸‚åœºåˆè§„
            (code_compliance > 90, 25),  # ä»£ç æ ¼å¼åˆè§„
            (len(results['industry_coverage']) > 5, 20)  # è¡Œä¸šè¦†ç›–
        ]
        
        results['a_share_compliance_score'] = sum(
            weight for condition, weight in compliance_factors if condition
        )
        
        logger.success(f"âœ… Aè‚¡å¸‚åœºç‰¹å¾éªŒè¯å®Œæˆ")
        logger.info(f"Aè‚¡åˆè§„æ€§è¯„åˆ†: {results['a_share_compliance_score']}/100")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ Aè‚¡å¸‚åœºç‰¹å¾éªŒè¯å¤±è´¥: {e}")
        return {}


def validate_t1_trading_suitability(data: pd.DataFrame) -> dict:
    """éªŒè¯T+1äº¤æ˜“é€‚ç”¨æ€§"""
    logger.info("â° éªŒè¯T+1äº¤æ˜“é€‚ç”¨æ€§")
    
    try:
        results = {
            'liquidity_score': 0,
            'volatility_distribution': {},
            'gap_risk_assessment': {},
            't1_suitability_score': 0
        }
        
        # 1. æµåŠ¨æ€§è¯„ä¼°
        logger.info("è¯„ä¼°æµåŠ¨æ€§...")
        if 'vol' in data.columns:
            avg_volume = data.groupby('ts_code')['vol'].mean()
            high_liquidity_stocks = (avg_volume > 1000000).sum()  # æ—¥å‡æˆäº¤é‡>100ä¸‡
            total_stocks = len(avg_volume)
            
            results['liquidity_score'] = (high_liquidity_stocks / total_stocks) * 100
            logger.info(f"é«˜æµåŠ¨æ€§è‚¡ç¥¨æ¯”ä¾‹: {results['liquidity_score']:.1f}%")
        
        # 2. æ³¢åŠ¨ç‡åˆ†å¸ƒ
        logger.info("åˆ†ææ³¢åŠ¨ç‡åˆ†å¸ƒ...")
        if 'pct_chg' in data.columns:
            volatility = data.groupby('ts_code')['pct_chg'].std()
            
            results['volatility_distribution'] = {
                'low_vol': (volatility < 3).sum(),      # ä½æ³¢åŠ¨ <3%
                'medium_vol': ((volatility >= 3) & (volatility < 6)).sum(),  # ä¸­æ³¢åŠ¨ 3-6%
                'high_vol': (volatility >= 6).sum()     # é«˜æ³¢åŠ¨ >6%
            }
            
            logger.info("æ³¢åŠ¨ç‡åˆ†å¸ƒ:")
            for vol_type, count in results['volatility_distribution'].items():
                logger.info(f"  {vol_type}: {count} åªè‚¡ç¥¨")
        
        # 3. è·³ç©ºé£é™©è¯„ä¼°
        logger.info("è¯„ä¼°è·³ç©ºé£é™©...")
        gap_risks = []
        
        for ts_code in data['ts_code'].unique():
            stock_data = data[data['ts_code'] == ts_code].sort_values('trade_date')
            if len(stock_data) > 1:
                stock_data = stock_data.copy()
                stock_data['gap'] = (
                    (stock_data['open'] - stock_data['close'].shift(1)) / 
                    stock_data['close'].shift(1) * 100
                ).abs()
                avg_gap = stock_data['gap'].mean()
                gap_risks.append(avg_gap)
        
        if gap_risks:
            results['gap_risk_assessment'] = {
                'avg_gap_risk': np.mean(gap_risks),
                'max_gap_risk': np.max(gap_risks),
                'low_gap_stocks': sum(1 for gap in gap_risks if gap < 1),  # è·³ç©º<1%
                'high_gap_stocks': sum(1 for gap in gap_risks if gap > 3)  # è·³ç©º>3%
            }
            
            logger.info(f"å¹³å‡è·³ç©ºé£é™©: {results['gap_risk_assessment']['avg_gap_risk']:.2f}%")
            logger.info(f"ä½è·³ç©ºé£é™©è‚¡ç¥¨: {results['gap_risk_assessment']['low_gap_stocks']} åª")
        
        # 4. T+1é€‚ç”¨æ€§è¯„åˆ†
        suitability_factors = [
            (results['liquidity_score'] > 70, 30),  # æµåŠ¨æ€§
            (results['volatility_distribution'].get('low_vol', 0) > 5, 25),  # ä½æ³¢åŠ¨è‚¡ç¥¨æ•°é‡
            (results['gap_risk_assessment'].get('avg_gap_risk', 10) < 2, 25),  # è·³ç©ºé£é™©
            (len(data['ts_code'].unique()) > 10, 20)  # è‚¡ç¥¨æ•°é‡
        ]
        
        results['t1_suitability_score'] = sum(
            weight for condition, weight in suitability_factors if condition
        )
        
        logger.success(f"âœ… T+1äº¤æ˜“é€‚ç”¨æ€§éªŒè¯å®Œæˆ")
        logger.info(f"T+1é€‚ç”¨æ€§è¯„åˆ†: {results['t1_suitability_score']}/100")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ T+1äº¤æ˜“é€‚ç”¨æ€§éªŒè¯å¤±è´¥: {e}")
        return {}


def generate_quality_report(completeness, accuracy, a_share_features, t1_suitability):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    logger.info("ğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š")
    
    try:
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scores = [
            completeness.get('completeness_score', 0),
            accuracy.get('accuracy_score', 0),
            a_share_features.get('a_share_compliance_score', 0),
            t1_suitability.get('t1_suitability_score', 0)
        ]
        
        overall_score = sum(scores) / len(scores)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 80)
        report.append("Aè‚¡æ•°æ®è´¨é‡éªŒè¯æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ç»¼åˆè¯„åˆ†
        report.append(f"ğŸ“Š ç»¼åˆè´¨é‡è¯„åˆ†: {overall_score:.1f}/100")
        report.append("")
        
        # è¯¦ç»†è¯„åˆ†
        report.append("ğŸ“‹ è¯¦ç»†è¯„åˆ†:")
        report.append(f"  æ•°æ®å®Œæ•´æ€§: {completeness.get('completeness_score', 0):.1f}/100")
        report.append(f"  æ•°æ®å‡†ç¡®æ€§: {accuracy.get('accuracy_score', 0):.1f}/100")
        report.append(f"  Aè‚¡åˆè§„æ€§: {a_share_features.get('a_share_compliance_score', 0):.1f}/100")
        report.append(f"  T+1é€‚ç”¨æ€§: {t1_suitability.get('t1_suitability_score', 0):.1f}/100")
        report.append("")
        
        # æ•°æ®æ¦‚å†µ
        report.append("ğŸ“ˆ æ•°æ®æ¦‚å†µ:")
        report.append(f"  æ€»è®°å½•æ•°: {completeness.get('total_records', 0):,}")
        report.append(f"  è‚¡ç¥¨æ•°é‡: {completeness.get('unique_stocks', 0)}")
        
        date_range = completeness.get('date_range', {})
        if date_range:
            report.append(f"  æ—¥æœŸèŒƒå›´: {date_range.get('start', 'N/A')} åˆ° {date_range.get('end', 'N/A')}")
        
        # è´¨é‡é—®é¢˜
        report.append("")
        report.append("âš ï¸ å‘ç°çš„é—®é¢˜:")
        
        issues = []
        if accuracy.get('price_logic_errors', 0) > 0:
            issues.append(f"ä»·æ ¼é€»è¾‘é”™è¯¯: {accuracy['price_logic_errors']} æ¡")
        if accuracy.get('volume_anomalies', 0) > 0:
            issues.append(f"æˆäº¤é‡å¼‚å¸¸: {accuracy['volume_anomalies']} æ¡")
        if a_share_features.get('limit_violations', 0) > 0:
            issues.append(f"æ¶¨è·Œåœè¿è§„: {a_share_features['limit_violations']} æ¡")
        
        if issues:
            for issue in issues:
                report.append(f"  â€¢ {issue}")
        else:
            report.append("  âœ… æœªå‘ç°é‡å¤§è´¨é‡é—®é¢˜")
        
        # å»ºè®®
        report.append("")
        report.append("ğŸ’¡ å»ºè®®:")
        
        if overall_score >= 90:
            report.append("  âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        elif overall_score >= 80:
            report.append("  âš ï¸ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿®å¤å‘ç°çš„é—®é¢˜")
        elif overall_score >= 70:
            report.append("  âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦æ”¹è¿›æ•°æ®æºæˆ–æ¸…æ´—æµç¨‹")
        else:
            report.append("  âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œä¸å»ºè®®ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        
        report.append("")
        report.append("ğŸ›ï¸ Aè‚¡T+1äº¤æ˜“ç‰¹ç‚¹:")
        report.append("  â€¢ å½“æ—¥ä¹°å…¥æ¬¡æ—¥æ‰èƒ½å–å‡º")
        report.append("  â€¢ æ¶¨è·Œåœé™åˆ¶ä¿æŠ¤æŠ•èµ„è€…")
        report.append("  â€¢ éœ€è¦å…³æ³¨éš”å¤œé£é™©")
        report.append("  â€¢ æµåŠ¨æ€§æ˜¯å…³é”®è€ƒè™‘å› ç´ ")
        
        report.append("=" * 80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        print("\n" + report_content)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        logs_dir = project_root / 'logs' / 'production'
        logs_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = logs_dir / f'a_share_data_quality_report_{timestamp}.txt'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.success(f"âœ… æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return overall_score >= 80  # 80åˆ†ä»¥ä¸Šè®¤ä¸ºè´¨é‡åˆæ ¼
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” Aè‚¡æ•°æ®è´¨é‡éªŒè¯ç³»ç»Ÿ")
    logger.info("=" * 80)
    
    try:
        # åŠ è½½æ•°æ®
        data_file = project_root / 'data' / 'production' / 'a_share' / 'a_share_daily_quotes.csv'
        
        if not data_file.exists():
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return False
        
        logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        data = pd.read_csv(data_file)
        
        # æ‰§è¡ŒéªŒè¯
        logger.info("\nğŸ¯ å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...")
        
        # 1. å®Œæ•´æ€§éªŒè¯
        logger.info("\nğŸ“Š æ­¥éª¤1: æ•°æ®å®Œæ•´æ€§éªŒè¯")
        completeness = validate_data_completeness(data)
        
        # 2. å‡†ç¡®æ€§éªŒè¯
        logger.info("\nğŸ¯ æ­¥éª¤2: æ•°æ®å‡†ç¡®æ€§éªŒè¯")
        accuracy = validate_data_accuracy(data)
        
        # 3. Aè‚¡ç‰¹å¾éªŒè¯
        logger.info("\nğŸ›ï¸ æ­¥éª¤3: Aè‚¡å¸‚åœºç‰¹å¾éªŒè¯")
        a_share_features = validate_a_share_characteristics(data)
        
        # 4. T+1é€‚ç”¨æ€§éªŒè¯
        logger.info("\nâ° æ­¥éª¤4: T+1äº¤æ˜“é€‚ç”¨æ€§éªŒè¯")
        t1_suitability = validate_t1_trading_suitability(data)
        
        # 5. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        logger.info("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š")
        quality_passed = generate_quality_report(
            completeness, accuracy, a_share_features, t1_suitability
        )
        
        if quality_passed:
            logger.success("ğŸ‰ Aè‚¡æ•°æ®è´¨é‡éªŒè¯é€šè¿‡ï¼")
            logger.info("âœ… æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        else:
            logger.warning("âš ï¸ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")
            logger.info("ğŸ”§ è¯·æ ¹æ®æŠ¥å‘Šå»ºè®®ä¼˜åŒ–æ•°æ®è´¨é‡")
        
        return quality_passed
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è´¨é‡éªŒè¯å¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
