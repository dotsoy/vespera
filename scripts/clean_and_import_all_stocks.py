#!/usr/bin/env python3
"""
æ¸…ç†æ•°æ®åº“æ¨¡æ‹Ÿæ•°æ®å¹¶å½•å…¥å…¨Aè‚¡çœŸå®æ•°æ®
"""
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import time
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import get_logger
from src.utils.database import get_db_manager
from src.data_sources.akshare_data_source import AkShareDataSource
from src.data_sources.base_data_source import DataRequest, DataType

logger = get_logger("clean_and_import")


def clean_existing_data():
    """æ¸…ç†æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®"""
    try:
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®")

        db = get_db_manager()

        # æ£€æŸ¥ç°æœ‰æ•°æ®
        try:
            check_query = """
            SELECT
                COUNT(*) as total_records,
                COUNT(DISTINCT ts_code) as total_stocks,
                MIN(trade_date) as earliest_date,
                MAX(trade_date) as latest_date
            FROM stock_daily_quotes
            """

            existing_data = db.execute_postgres_query(check_query)
            logger.info(f"ç°æœ‰æ•°æ®ç»Ÿè®¡: {existing_data.iloc[0].to_dict()}")
        except Exception as e:
            logger.info(f"æ£€æŸ¥ç°æœ‰æ•°æ®æ—¶å‡ºé”™: {e}")

        # ä½¿ç”¨ pandas çš„æ–¹å¼æ¸…ç†æ•°æ® (é€šè¿‡åˆ›å»ºç©ºè¡¨)
        logger.info("æ¸…ç†è‚¡ç¥¨æ—¥çº¿æ•°æ®...")
        try:
            # åˆ›å»ºç©ºçš„ DataFrame å¹¶è¦†ç›–è¡¨
            empty_df = pd.DataFrame(columns=['ts_code', 'trade_date', 'open_price', 'high_price', 'low_price', 'close_price', 'vol', 'pct_chg'])
            empty_df.to_sql('stock_daily_quotes', db.postgres_engine, if_exists='replace', index=False)
            logger.info("âœ… è‚¡ç¥¨æ—¥çº¿æ•°æ®è¡¨å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"æ¸…ç†æ—¥çº¿æ•°æ®æ—¶å‡ºé”™: {e}")

        # æ¸…ç†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        logger.info("æ¸…ç†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
        try:
            empty_basic = pd.DataFrame(columns=['ts_code', 'symbol', 'name', 'area', 'industry', 'market', 'list_date', 'list_status'])
            empty_basic.to_sql('stock_basic', db.postgres_engine, if_exists='replace', index=False)
            logger.info("âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è¡¨å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"æ¸…ç†åŸºç¡€ä¿¡æ¯æ—¶å‡ºé”™: {e}")

        logger.success("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
        return False


def get_all_a_stocks():
    """è·å–å…¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
    try:
        logger.info("ğŸ“‹ è·å–å…¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨")

        akshare_client = AkShareDataSource()

        # åˆå§‹åŒ–æ•°æ®æº
        if not akshare_client.initialize():
            logger.error("âŒ AkShareæ•°æ®æºåˆå§‹åŒ–å¤±è´¥")
            return pd.DataFrame()

        # è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        request = DataRequest(data_type=DataType.STOCK_BASIC)
        response = akshare_client.fetch_data(request)

        if not response.success or response.data.empty:
            logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
            return pd.DataFrame()

        stock_basic = response.data

        # è¿‡æ»¤Aè‚¡ (æ’é™¤é€€å¸‚ã€STç­‰)
        a_stocks = stock_basic[
            (stock_basic['market'].isin(['æ·±äº¤æ‰€ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ä¸Šäº¤æ‰€ä¸»æ¿', 'ç§‘åˆ›æ¿', 'åŒ—äº¤æ‰€']))
        ].copy()

        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼Œæ’é™¤é€€å¸‚è‚¡ç¥¨
        a_stocks = a_stocks[
            (~a_stocks['name'].str.contains('ST', na=False)) &
            (~a_stocks['name'].str.contains('é€€', na=False)) &
            (a_stocks['list_status'] == 'L')  # ä¸Šå¸‚çŠ¶æ€
        ].copy()

        logger.info(f"âœ… è·å–åˆ° {len(a_stocks)} åªAè‚¡è‚¡ç¥¨")
        return a_stocks

    except Exception as e:
        logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        return pd.DataFrame()


def import_stock_basic_info(stock_basic_df: pd.DataFrame):
    """å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
    try:
        logger.info("ğŸ“Š å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
        
        db = get_db_manager()
        
        # å‡†å¤‡æ•°æ®
        stock_data = stock_basic_df[[
            'ts_code', 'symbol', 'name', 'area', 'industry', 
            'market', 'list_date', 'list_status'
        ]].copy()
        
        # æ‰¹é‡æ’å…¥
        success_count = 0
        batch_size = 100
        
        for i in range(0, len(stock_data), batch_size):
            batch = stock_data.iloc[i:i+batch_size]
            
            try:
                # ä½¿ç”¨ pandas to_sql æ–¹æ³•æ‰¹é‡æ’å…¥
                batch.to_sql(
                    'stock_basic', 
                    db.postgres_engine, 
                    if_exists='append', 
                    index=False,
                    method='multi'
                )
                success_count += len(batch)
                logger.info(f"å·²å¯¼å…¥ {success_count}/{len(stock_data)} åªè‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
                
            except Exception as e:
                logger.error(f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")
                continue
        
        logger.success(f"âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¯¼å…¥å®Œæˆï¼ŒæˆåŠŸå¯¼å…¥ {success_count} åªè‚¡ç¥¨")
        return success_count
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
        return 0


def import_stock_daily_data(stock_list: pd.DataFrame, days: int = 60):
    """å¯¼å…¥è‚¡ç¥¨æ—¥çº¿æ•°æ®"""
    try:
        logger.info(f"ğŸ“ˆ å¼€å§‹å¯¼å…¥è‚¡ç¥¨æ—¥çº¿æ•°æ® (æœ€è¿‘{days}å¤©)")

        akshare_client = AkShareDataSource()
        db = get_db_manager()

        # åˆå§‹åŒ–æ•°æ®æº
        if not akshare_client.initialize():
            logger.error("âŒ AkShareæ•°æ®æºåˆå§‹åŒ–å¤±è´¥")
            return 0, 0

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = datetime.now() - timedelta(days=days*2)  # å¤šå–ä¸€äº›å¤©æ•°ç¡®ä¿æœ‰è¶³å¤Ÿäº¤æ˜“æ—¥

        logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")

        total_stocks = len(stock_list)
        success_count = 0
        error_count = 0
        total_records = 0

        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
        batch_size = 20  # æ¯æ‰¹å¤„ç†20åªè‚¡ç¥¨ï¼ŒAkShareé¢‘ç‡é™åˆ¶è¾ƒä¸¥

        for batch_start in range(0, total_stocks, batch_size):
            batch_end = min(batch_start + batch_size, total_stocks)
            batch_stocks = stock_list.iloc[batch_start:batch_end]

            logger.info(f"å¤„ç†ç¬¬ {batch_start//batch_size + 1} æ‰¹è‚¡ç¥¨ ({batch_start+1}-{batch_end}/{total_stocks})")

            batch_data = []

            for _, stock in batch_stocks.iterrows():
                ts_code = stock['ts_code']

                try:
                    # è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
                    request = DataRequest(
                        data_type=DataType.DAILY_QUOTES,
                        symbol=ts_code,
                        start_date=start_date,
                        end_date=end_date
                    )
                    response = akshare_client.fetch_data(request)

                    if response.success and not response.data.empty:
                        daily_data = response.data
                        # åªä¿ç•™æœ€è¿‘çš„äº¤æ˜“æ—¥
                        daily_data = daily_data.sort_values('trade_date').tail(days)
                        batch_data.append(daily_data)
                        success_count += 1
                        total_records += len(daily_data)

                        if success_count % 10 == 0:
                            logger.info(f"å·²å¤„ç† {success_count} åªè‚¡ç¥¨ï¼Œç´¯è®¡ {total_records} æ¡è®°å½•")
                    else:
                        logger.warning(f"è‚¡ç¥¨ {ts_code} æ— æ•°æ®")
                        error_count += 1

                    # æ§åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼ŒAkShareéœ€è¦æ›´é•¿é—´éš”
                    time.sleep(0.2)

                except Exception as e:
                    logger.error(f"è·å–è‚¡ç¥¨ {ts_code} æ•°æ®å¤±è´¥: {e}")
                    error_count += 1
                    continue
            
            # æ‰¹é‡æ’å…¥æ•°æ®åº“
            if batch_data:
                try:
                    combined_batch = pd.concat(batch_data, ignore_index=True)
                    
                    # æ’å…¥æ•°æ®åº“
                    combined_batch.to_sql(
                        'stock_daily_quotes',
                        db.postgres_engine,
                        if_exists='append',
                        index=False,
                        method='multi'
                    )
                    
                    logger.info(f"âœ… ç¬¬ {batch_start//batch_size + 1} æ‰¹æ•°æ®æ’å…¥å®Œæˆï¼Œ{len(combined_batch)} æ¡è®°å½•")
                    
                except Exception as e:
                    logger.error(f"âŒ ç¬¬ {batch_start//batch_size + 1} æ‰¹æ•°æ®æ’å…¥å¤±è´¥: {e}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            time.sleep(1)
        
        logger.success(f"ğŸ‰ è‚¡ç¥¨æ—¥çº¿æ•°æ®å¯¼å…¥å®Œæˆï¼")
        logger.info(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        logger.info(f"  æˆåŠŸè‚¡ç¥¨: {success_count}")
        logger.info(f"  å¤±è´¥è‚¡ç¥¨: {error_count}")
        logger.info(f"  æ€»è®°å½•æ•°: {total_records}")
        logger.info(f"  æˆåŠŸç‡: {success_count/(success_count+error_count)*100:.1f}%")
        
        return success_count, total_records
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è‚¡ç¥¨æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
        return 0, 0


def verify_import_results():
    """éªŒè¯å¯¼å…¥ç»“æœ"""
    try:
        logger.info("ğŸ” éªŒè¯å¯¼å…¥ç»“æœ")
        
        db = get_db_manager()
        
        # æ£€æŸ¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        basic_query = """
        SELECT 
            COUNT(*) as total_stocks,
            COUNT(DISTINCT market) as markets,
            COUNT(DISTINCT industry) as industries
        FROM stock_basic
        """
        
        basic_stats = db.execute_postgres_query(basic_query)
        logger.info(f"è‚¡ç¥¨åŸºç¡€ä¿¡æ¯: {basic_stats.iloc[0].to_dict()}")
        
        # æ£€æŸ¥æ—¥çº¿æ•°æ®
        daily_query = """
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT ts_code) as stocks_with_data,
            MIN(trade_date) as earliest_date,
            MAX(trade_date) as latest_date,
            AVG(vol) as avg_volume
        FROM stock_daily_quotes
        """
        
        daily_stats = db.execute_postgres_query(daily_query)
        logger.info(f"æ—¥çº¿æ•°æ®ç»Ÿè®¡: {daily_stats.iloc[0].to_dict()}")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        completeness_query = """
        SELECT 
            sb.market,
            COUNT(sb.ts_code) as total_stocks,
            COUNT(DISTINCT sdq.ts_code) as stocks_with_data,
            ROUND(COUNT(DISTINCT sdq.ts_code) * 100.0 / COUNT(sb.ts_code), 2) as data_completeness_pct
        FROM stock_basic sb
        LEFT JOIN stock_daily_quotes sdq ON sb.ts_code = sdq.ts_code
        GROUP BY sb.market
        ORDER BY data_completeness_pct DESC
        """
        
        completeness = db.execute_postgres_query(completeness_query)
        logger.info("å„å¸‚åœºæ•°æ®å®Œæ•´æ€§:")
        for _, row in completeness.iterrows():
            logger.info(f"  {row['market']}: {row['stocks_with_data']}/{row['total_stocks']} ({row['data_completeness_pct']}%)")
        
        # æ£€æŸ¥æœ€æ–°æ•°æ®æ—¥æœŸ
        latest_query = """
        SELECT 
            trade_date,
            COUNT(*) as stock_count
        FROM stock_daily_quotes
        WHERE trade_date >= (SELECT MAX(trade_date) FROM stock_daily_quotes)
        GROUP BY trade_date
        ORDER BY trade_date DESC
        LIMIT 5
        """
        
        latest_data = db.execute_postgres_query(latest_query)
        logger.info("æœ€æ–°äº¤æ˜“æ—¥æ•°æ®:")
        for _, row in latest_data.iterrows():
            logger.info(f"  {row['trade_date']}: {row['stock_count']} åªè‚¡ç¥¨")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¯¼å…¥ç»“æœå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ¸…ç†æ•°æ®åº“å¹¶å¯¼å…¥å…¨Aè‚¡æ•°æ®")
    logger.info("=" * 60)
    
    try:
        # æ­¥éª¤1: æ¸…ç†ç°æœ‰æ•°æ®
        logger.info("ğŸ“‹ æ­¥éª¤1: æ¸…ç†ç°æœ‰æ•°æ®")
        if not clean_existing_data():
            logger.error("âŒ æ•°æ®æ¸…ç†å¤±è´¥ï¼Œé€€å‡º")
            return False
        
        # æ­¥éª¤2: è·å–å…¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨
        logger.info("ğŸ“‹ æ­¥éª¤2: è·å–å…¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨")
        stock_list = get_all_a_stocks()
        
        if stock_list.empty:
            logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œé€€å‡º")
            return False
        
        logger.info(f"âœ… è·å–åˆ° {len(stock_list)} åªAè‚¡è‚¡ç¥¨")
        
        # æ­¥éª¤3: å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        logger.info("ğŸ“‹ æ­¥éª¤3: å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
        basic_count = import_stock_basic_info(stock_list)
        
        if basic_count == 0:
            logger.error("âŒ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¯¼å…¥å¤±è´¥ï¼Œé€€å‡º")
            return False
        
        # æ­¥éª¤4: å¯¼å…¥è‚¡ç¥¨æ—¥çº¿æ•°æ®
        logger.info("ğŸ“‹ æ­¥éª¤4: å¯¼å…¥è‚¡ç¥¨æ—¥çº¿æ•°æ®")
        
        # å…ˆå¯¼å…¥å°‘é‡è‚¡ç¥¨æµ‹è¯•
        test_stocks = stock_list.head(100)  # å…ˆå¯¼å…¥100åªè‚¡ç¥¨æµ‹è¯•
        logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å…ˆå¯¼å…¥ {len(test_stocks)} åªè‚¡ç¥¨")
        
        success_count, total_records = import_stock_daily_data(test_stocks, days=60)
        
        if success_count == 0:
            logger.error("âŒ è‚¡ç¥¨æ—¥çº¿æ•°æ®å¯¼å…¥å¤±è´¥ï¼Œé€€å‡º")
            return False
        
        # æ­¥éª¤5: éªŒè¯å¯¼å…¥ç»“æœ
        logger.info("ğŸ“‹ æ­¥éª¤5: éªŒè¯å¯¼å…¥ç»“æœ")
        if not verify_import_results():
            logger.warning("âš ï¸ å¯¼å…¥ç»“æœéªŒè¯æœ‰é—®é¢˜")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¯¼å…¥å‰©ä½™è‚¡ç¥¨
        remaining_stocks = len(stock_list) - len(test_stocks)
        if remaining_stocks > 0:
            logger.info(f"ğŸ¤” æµ‹è¯•å¯¼å…¥å®Œæˆï¼Œè¿˜æœ‰ {remaining_stocks} åªè‚¡ç¥¨å¾…å¯¼å…¥")
            logger.info("å¦‚éœ€å¯¼å…¥å…¨éƒ¨è‚¡ç¥¨ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ test_stocks = stock_list.head(100) ä¸º test_stocks = stock_list")
        
        logger.success("ğŸ‰ æ•°æ®å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info("ğŸ“Š å¯¼å…¥æ€»ç»“:")
        logger.info(f"  è‚¡ç¥¨åŸºç¡€ä¿¡æ¯: {basic_count} åª")
        logger.info(f"  æ—¥çº¿æ•°æ®è‚¡ç¥¨: {success_count} åª")
        logger.info(f"  æ—¥çº¿æ•°æ®è®°å½•: {total_records} æ¡")
        logger.info(f"  å¹³å‡æ¯è‚¡è®°å½•: {total_records/max(success_count,1):.1f} æ¡")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¯¼å…¥ä»»åŠ¡å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
