#!/usr/bin/env python3
"""
ç”Ÿäº§æ•°æ®å¯¼å…¥è„šæœ¬
ä½¿ç”¨AllTickå’ŒAlpha Vantageå¯¼å…¥çœŸå®çš„è‚¡ç¥¨æ•°æ®
æ’é™¤STè‚¡ç¥¨å’ŒåŒ—äº¤æ‰€è‚¡ç¥¨
"""
import sys
import os
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import time
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import get_logger
from src.utils.stock_filter import StockFilter
from src.data_sources.base_data_source import DataRequest, DataType
from src.data_sources.alltick_data_source import AllTickDataSource
from src.data_sources.alpha_vantage_data_source import AlphaVantageDataSource
from src.data_sources.data_source_manager import DataSourceManager

logger = get_logger("import_production_data")

# ç”Ÿäº§ç¯å¢ƒtoken
ALLTICK_TOKEN = "5d77b3af30d6b74b6bad3340996cb399-c-app"
ALPHA_VANTAGE_API_KEY = "3SHZ17DOQBH5X6IX"


def setup_production_data_sources():
    """é…ç½®ç”Ÿäº§æ•°æ®æº"""
    logger.info("ğŸ”§ é…ç½®ç”Ÿäº§æ•°æ®æº")
    
    try:
        # åˆ›å»ºæ•°æ®æºç®¡ç†å™¨
        manager = DataSourceManager()
        
        # é…ç½®AllTickæ•°æ®æº
        logger.info("é…ç½®AllTickæ•°æ®æº...")
        alltick_config = {
            'priority': 1,
            'rate_limit': 100,
            'timeout': 30,
            'description': 'AllTickç”Ÿäº§ç¯å¢ƒ'
        }
        
        alltick_source = AllTickDataSource(ALLTICK_TOKEN, alltick_config)
        if alltick_source.initialize():
            manager.register_data_source(alltick_source)
            logger.success("âœ… AllTickæ•°æ®æºé…ç½®æˆåŠŸ")
        else:
            logger.error("âŒ AllTickæ•°æ®æºåˆå§‹åŒ–å¤±è´¥")
            return None
        
        # é…ç½®Alpha Vantageæ•°æ®æº
        logger.info("é…ç½®Alpha Vantageæ•°æ®æº...")
        alpha_config = {
            'priority': 2,
            'rate_limit': 5,
            'timeout': 30,
            'description': 'Alpha Vantageç”Ÿäº§ç¯å¢ƒ'
        }
        
        alpha_source = AlphaVantageDataSource(ALPHA_VANTAGE_API_KEY, alpha_config)
        if alpha_source.initialize():
            manager.register_data_source(alpha_source)
            logger.success("âœ… Alpha Vantageæ•°æ®æºé…ç½®æˆåŠŸ")
        else:
            logger.warning("âš ï¸ Alpha Vantageæ•°æ®æºåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨AllTick")
        
        return manager
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æºé…ç½®å¤±è´¥: {e}")
        return None


def get_stock_list(data_manager):
    """è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œæ’é™¤STè‚¡ç¥¨å’ŒåŒ—äº¤æ‰€"""
    logger.info("ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨")
    
    try:
        # ä½¿ç”¨AllTickè·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        request = DataRequest(
            data_type=DataType.STOCK_BASIC,
            extra_params={
                'market': 'cn',
                'type': 'stock'
            }
        )
        
        response = data_manager.get_data(request)
        
        if not response.success:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {response.error_message}")
            return pd.DataFrame()
        
        stock_data = response.data
        logger.info(f"è·å–åˆ° {len(stock_data)} åªè‚¡ç¥¨")

        # ä½¿ç”¨è‚¡ç¥¨è¿‡æ»¤å™¨è¿›è¡Œè¿‡æ»¤
        stock_filter = StockFilter()

        # ç¡®å®šåˆ—å
        code_column = 'ts_code' if 'ts_code' in stock_data.columns else 'symbol'
        name_column = 'name' if 'name' in stock_data.columns else 'name'

        # åº”ç”¨è¿‡æ»¤å™¨
        stock_data = stock_filter.filter_stock_list(
            stock_data,
            code_column=code_column,
            name_column=name_column
        )
        
        logger.success(f"âœ… æœ€ç»ˆè‚¡ç¥¨åˆ—è¡¨: {len(stock_data)} åª")
        return stock_data
        
    except Exception as e:
        logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        return pd.DataFrame()


def import_stock_basic_info(data_manager, stock_list):
    """å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
    logger.info("ğŸ“Š å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
    
    try:
        if stock_list.empty:
            logger.error("âŒ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return False
        
        # è¿™é‡Œåº”è¯¥å°†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
        # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦æ ¹æ®å®é™…æ•°æ®åº“è¿æ¥è°ƒæ•´ï¼‰:
        """
        from src.utils.database import get_db_manager
        
        db_manager = get_db_manager()
        
        # ä¿å­˜åˆ°PostgreSQL
        db_manager.insert_dataframe_to_postgres(
            stock_list, 
            'stock_basic', 
            if_exists='replace'
        )
        """
        
        # ä¸´æ—¶å®ç°ï¼šä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        output_dir = project_root / 'data' / 'production'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / 'stock_basic.csv'
        stock_list.to_csv(output_file, index=False, encoding='utf-8')
        
        logger.success(f"âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
        logger.info(f"å…±ä¿å­˜ {len(stock_list)} åªè‚¡ç¥¨çš„åŸºç¡€ä¿¡æ¯")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
        return False


def import_historical_data(data_manager, stock_list, days=30):
    """å¯¼å…¥å†å²æ•°æ®"""
    logger.info(f"ğŸ“ˆ å¯¼å…¥æœ€è¿‘ {days} å¤©çš„å†å²æ•°æ®")
    
    try:
        if stock_list.empty:
            logger.error("âŒ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return False
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days)
        
        logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ä»¥é¿å…APIé™åˆ¶
        max_stocks = 50  # é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        if len(stock_list) > max_stocks:
            logger.warning(f"âš ï¸ è‚¡ç¥¨æ•°é‡è¿‡å¤šï¼Œä»…å¤„ç†å‰ {max_stocks} åªè‚¡ç¥¨")
            stock_list = stock_list.head(max_stocks)
        
        all_data = []
        success_count = 0
        total_count = len(stock_list)
        
        for idx, (_, stock) in enumerate(stock_list.iterrows(), 1):
            stock_code = stock.get('ts_code', stock.get('symbol', ''))
            stock_name = stock.get('name', '')
            
            logger.info(f"[{idx}/{total_count}] è·å– {stock_code} ({stock_name}) çš„å†å²æ•°æ®")
            
            try:
                request = DataRequest(
                    data_type=DataType.DAILY_QUOTES,
                    symbol=stock_code,
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d')
                )
                
                response = data_manager.get_data(request)
                
                if response.success and not response.data.empty:
                    all_data.append(response.data)
                    success_count += 1
                    logger.success(f"âœ… è·å–æˆåŠŸï¼Œ{len(response.data)} æ¡è®°å½•")
                else:
                    logger.warning(f"âš ï¸ è·å–å¤±è´¥: {response.error_message}")
                
                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(0.6)  # AllTické™åˆ¶
                
            except Exception as e:
                logger.warning(f"âš ï¸ è·å– {stock_code} æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            
            # ä¿å­˜æ•°æ®
            output_dir = project_root / 'data' / 'production'
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / 'daily_quotes.csv'
            combined_data.to_csv(output_file, index=False, encoding='utf-8')
            
            logger.success(f"âœ… å†å²æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"æˆåŠŸè·å– {success_count}/{total_count} åªè‚¡ç¥¨çš„æ•°æ®")
            logger.info(f"æ€»è®°å½•æ•°: {len(combined_data)}")
            
            # è¿™é‡Œåº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“
            # ç¤ºä¾‹ä»£ç :
            """
            from src.utils.database import get_db_manager
            
            db_manager = get_db_manager()
            
            # ä¿å­˜åˆ°PostgreSQL
            db_manager.insert_dataframe_to_postgres(
                combined_data, 
                'daily_quotes', 
                if_exists='append'
            )
            
            # ä¿å­˜åˆ°ClickHouseï¼ˆç”¨äºåˆ†æï¼‰
            db_manager.insert_dataframe_to_clickhouse(
                combined_data,
                'stock_quotes_daily'
            )
            """
            
            return True
        else:
            logger.error("âŒ æœªè·å–åˆ°ä»»ä½•å†å²æ•°æ®")
            return False
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥å†å²æ•°æ®å¤±è´¥: {e}")
        return False


def validate_imported_data():
    """éªŒè¯å¯¼å…¥çš„æ•°æ®"""
    logger.info("ğŸ” éªŒè¯å¯¼å…¥çš„æ•°æ®")
    
    try:
        data_dir = project_root / 'data' / 'production'
        
        # æ£€æŸ¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        basic_file = data_dir / 'stock_basic.csv'
        if basic_file.exists():
            basic_data = pd.read_csv(basic_file)
            logger.success(f"âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯: {len(basic_data)} æ¡è®°å½•")
            
            # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
            if not basic_data.empty:
                logger.info("è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ ·ä¾‹:")
                print(basic_data.head().to_string())
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ–‡ä»¶")
        
        # æ£€æŸ¥å†å²æ•°æ®
        quotes_file = data_dir / 'daily_quotes.csv'
        if quotes_file.exists():
            quotes_data = pd.read_csv(quotes_file)
            logger.success(f"âœ… å†å²è¡Œæƒ…æ•°æ®: {len(quotes_data)} æ¡è®°å½•")
            
            # æ•°æ®ç»Ÿè®¡
            if not quotes_data.empty:
                unique_stocks = quotes_data['ts_code'].nunique() if 'ts_code' in quotes_data.columns else 0
                date_range = None
                if 'trade_date' in quotes_data.columns:
                    date_range = f"{quotes_data['trade_date'].min()} åˆ° {quotes_data['trade_date'].max()}"
                
                logger.info(f"æ¶µç›–è‚¡ç¥¨æ•°: {unique_stocks}")
                if date_range:
                    logger.info(f"æ—¥æœŸèŒƒå›´: {date_range}")
                
                # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
                logger.info("å†å²æ•°æ®æ ·ä¾‹:")
                print(quotes_data.head().to_string())
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å†å²è¡Œæƒ…æ•°æ®æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å¯¼å…¥ç”Ÿäº§æ•°æ®")
    logger.info("=" * 80)
    
    logger.info("é…ç½®ä¿¡æ¯:")
    logger.info(f"  AllTick Token: {ALLTICK_TOKEN[:20]}...")
    logger.info(f"  Alpha Vantage Key: {ALPHA_VANTAGE_API_KEY[:10]}...")
    logger.info("  è¿‡æ»¤æ¡ä»¶: æ’é™¤STè‚¡ç¥¨å’ŒåŒ—äº¤æ‰€")
    
    # æ‰§è¡Œå¯¼å…¥æ­¥éª¤
    steps = [
        ("é…ç½®ç”Ÿäº§æ•°æ®æº", lambda: setup_production_data_sources()),
        ("è·å–è‚¡ç¥¨åˆ—è¡¨", None),  # éœ€è¦æ•°æ®ç®¡ç†å™¨å‚æ•°
        ("å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯", None),  # éœ€è¦å‚æ•°
        ("å¯¼å…¥å†å²æ•°æ®", None),  # éœ€è¦å‚æ•°
        ("éªŒè¯å¯¼å…¥æ•°æ®", validate_imported_data)
    ]
    
    data_manager = None
    stock_list = pd.DataFrame()
    
    try:
        # 1. é…ç½®æ•°æ®æº
        logger.info("\nğŸ¯ æ­¥éª¤1: é…ç½®ç”Ÿäº§æ•°æ®æº")
        data_manager = setup_production_data_sources()
        if not data_manager:
            logger.error("âŒ æ•°æ®æºé…ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # 2. è·å–è‚¡ç¥¨åˆ—è¡¨
        logger.info("\nğŸ¯ æ­¥éª¤2: è·å–è‚¡ç¥¨åˆ—è¡¨")
        stock_list = get_stock_list(data_manager)
        if stock_list.empty:
            logger.error("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # 3. å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        logger.info("\nğŸ¯ æ­¥éª¤3: å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
        if not import_stock_basic_info(data_manager, stock_list):
            logger.error("âŒ å¯¼å…¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥")
            return False
        
        # 4. å¯¼å…¥å†å²æ•°æ®
        logger.info("\nğŸ¯ æ­¥éª¤4: å¯¼å…¥å†å²æ•°æ®")
        if not import_historical_data(data_manager, stock_list):
            logger.error("âŒ å¯¼å…¥å†å²æ•°æ®å¤±è´¥")
            return False
        
        # 5. éªŒè¯æ•°æ®
        logger.info("\nğŸ¯ æ­¥éª¤5: éªŒè¯å¯¼å…¥æ•°æ®")
        if not validate_imported_data():
            logger.warning("âš ï¸ æ•°æ®éªŒè¯å¤±è´¥")
        
        logger.success("ğŸ‰ ç”Ÿäº§æ•°æ®å¯¼å…¥å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
