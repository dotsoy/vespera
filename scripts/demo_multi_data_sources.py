#!/usr/bin/env python3
"""
å¤šæ•°æ®æºä¸­é—´å±‚æ¼”ç¤ºè„šæœ¬
å±•ç¤ºç»Ÿä¸€æ•°æ®æ¥å£çš„åŠŸèƒ½å’Œä¼˜åŠ¿
"""
import sys
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import time
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import get_logger
from src.data_sources.base_data_source import DataRequest, DataType
from src.data_sources.data_source_factory import get_data_service
from src.data_sources.data_fusion_engine import FusionStrategy, ValidationLevel

logger = get_logger("multi_data_sources_demo")


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ç”¨æ³•"""
    logger.info("ğŸš€ æ¼”ç¤º1: åŸºæœ¬æ•°æ®è·å–")
    logger.info("=" * 60)
    
    try:
        # è·å–æ•°æ®æœåŠ¡
        data_service = get_data_service(enable_cache=True)
        
        # åˆ›å»ºæ•°æ®è¯·æ±‚
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-01-01",
            end_date="2024-01-31"
        )
        
        # è·å–æ•°æ®
        logger.info("ğŸ“Š è¯·æ±‚è‹¹æœå…¬å¸(AAPL)2024å¹´1æœˆçš„æ—¥çº¿æ•°æ®")
        response = data_service.get_data(request)
        
        if response.success:
            logger.success(f"âœ… æ•°æ®è·å–æˆåŠŸï¼")
            logger.info(f"æ•°æ®æº: {response.source}")
            logger.info(f"æ•°æ®è®°å½•æ•°: {len(response.data)}")
            logger.info(f"æ•°æ®åˆ—: {list(response.data.columns)}")
            
            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
            if not response.data.empty:
                logger.info("å‰5è¡Œæ•°æ®:")
                print(response.data.head().to_string())
        else:
            logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {response.error_message}")
        
        return response.success
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º1å¤±è´¥: {e}")
        return False


def demo_multiple_sources():
    """æ¼”ç¤ºå¤šæ•°æ®æºè·å–å’Œèåˆ"""
    logger.info("\nğŸ”„ æ¼”ç¤º2: å¤šæ•°æ®æºèåˆ")
    logger.info("=" * 60)
    
    try:
        data_service = get_data_service(enable_cache=True)
        
        # è·å–å¯ç”¨æ•°æ®æº
        available_sources = data_service.get_available_sources(DataType.DAILY_QUOTES)
        logger.info(f"å¯ç”¨æ•°æ®æº: {available_sources}")
        
        if len(available_sources) < 2:
            logger.warning("âš ï¸ å¯ç”¨æ•°æ®æºä¸è¶³ï¼Œæ— æ³•æ¼”ç¤ºå¤šæºèåˆ")
            return True
        
        # åˆ›å»ºæ•°æ®è¯·æ±‚
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-06-01",
            end_date="2024-06-07"
        )
        
        # ä½¿ç”¨ä¸åŒçš„èåˆç­–ç•¥
        strategies = [
            ('first_success', 'ç¬¬ä¸€ä¸ªæˆåŠŸ'),
            ('quality_based', 'åŸºäºè´¨é‡'),
            ('weighted_average', 'åŠ æƒå¹³å‡')
        ]
        
        for strategy, description in strategies:
            logger.info(f"\nğŸ“ˆ ä½¿ç”¨ {description} ç­–ç•¥è·å–æ•°æ®")
            
            response = data_service.get_data(
                request=request,
                merge_strategy=strategy,
                use_cache=False  # ç¦ç”¨ç¼“å­˜ä»¥æµ‹è¯•ä¸åŒç­–ç•¥
            )
            
            if response.success:
                logger.success(f"âœ… {description} ç­–ç•¥æˆåŠŸ")
                logger.info(f"æ•°æ®æº: {response.source}")
                logger.info(f"è®°å½•æ•°: {len(response.data)}")
                
                if hasattr(response, 'metadata') and response.metadata:
                    logger.info(f"å…ƒæ•°æ®: {response.metadata}")
            else:
                logger.error(f"âŒ {description} ç­–ç•¥å¤±è´¥: {response.error_message}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º2å¤±è´¥: {e}")
        return False


def demo_caching():
    """æ¼”ç¤ºç¼“å­˜åŠŸèƒ½"""
    logger.info("\nğŸ’¾ æ¼”ç¤º3: ç¼“å­˜åŠŸèƒ½")
    logger.info("=" * 60)
    
    try:
        data_service = get_data_service(enable_cache=True)
        
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-05-01",
            end_date="2024-05-31"
        )
        
        # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆä»æ•°æ®æºè·å–ï¼‰
        logger.info("ğŸ“Š ç¬¬ä¸€æ¬¡è¯·æ±‚æ•°æ®ï¼ˆä»æ•°æ®æºï¼‰")
        start_time = time.time()
        response1 = data_service.get_data(request, use_cache=True)
        time1 = time.time() - start_time
        
        if response1.success:
            logger.success(f"âœ… ç¬¬ä¸€æ¬¡è¯·æ±‚æˆåŠŸï¼Œè€—æ—¶: {time1:.2f}ç§’")
            logger.info(f"æ•°æ®æº: {response1.source}")
            logger.info(f"è®°å½•æ•°: {len(response1.data)}")
        
        # ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆä»ç¼“å­˜è·å–ï¼‰
        logger.info("\nğŸ“Š ç¬¬äºŒæ¬¡è¯·æ±‚ç›¸åŒæ•°æ®ï¼ˆä»ç¼“å­˜ï¼‰")
        start_time = time.time()
        response2 = data_service.get_data(request, use_cache=True)
        time2 = time.time() - start_time
        
        if response2.success:
            logger.success(f"âœ… ç¬¬äºŒæ¬¡è¯·æ±‚æˆåŠŸï¼Œè€—æ—¶: {time2:.2f}ç§’")
            logger.info(f"æ•°æ®æº: {response2.source}")
            logger.info(f"è®°å½•æ•°: {len(response2.data)}")
            
            # è®¡ç®—æ€§èƒ½æå‡
            if time1 > 0:
                speedup = time1 / time2 if time2 > 0 else float('inf')
                logger.info(f"ğŸš€ ç¼“å­˜æ€§èƒ½æå‡: {speedup:.1f}å€")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        health_status = data_service.health_check()
        if 'cache' in health_status:
            cache_stats = health_status['cache']
            logger.info(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
            logger.info(f"  å†…å­˜ç¼“å­˜: {cache_stats.get('memory_cache_size', 0)} é¡¹")
            logger.info(f"  ç£ç›˜ç¼“å­˜: {cache_stats.get('disk_cache_files', 0)} æ–‡ä»¶")
            logger.info(f"  æ•°æ®åº“ç¼“å­˜: {cache_stats.get('database_cache_records', 0)} è®°å½•")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º3å¤±è´¥: {e}")
        return False


def demo_fault_tolerance():
    """æ¼”ç¤ºæ•…éšœå®¹é”™"""
    logger.info("\nğŸ›¡ï¸ æ¼”ç¤º4: æ•…éšœå®¹é”™")
    logger.info("=" * 60)
    
    try:
        data_service = get_data_service(enable_cache=True)
        
        # è·å–å¥åº·çŠ¶æ€
        health_status = data_service.health_check()
        logger.info("ğŸ“Š æ•°æ®æºå¥åº·çŠ¶æ€:")
        
        for source_name, status in health_status.get('sources', {}).items():
            status_emoji = "âœ…" if status.get('status') == 'available' else "âŒ"
            logger.info(f"  {status_emoji} {source_name}: {status.get('status', 'unknown')}")
        
        # æµ‹è¯•æ•…éšœè½¬ç§»
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-04-01",
            end_date="2024-04-07"
        )
        
        logger.info("\nğŸ”„ æµ‹è¯•æ•…éšœè½¬ç§»æœºåˆ¶")
        
        # æŒ‡å®šä¸€ä¸ªå¯èƒ½ä¸å¯ç”¨çš„æ•°æ®æºä½œä¸ºé¦–é€‰
        preferred_sources = ["NonExistentSource", "Tushare", "Yahoo Finance"]
        
        response = data_service.get_data(
            request=request,
            preferred_sources=preferred_sources,
            fallback=True
        )
        
        if response.success:
            logger.success("âœ… æ•…éšœè½¬ç§»æˆåŠŸ")
            logger.info(f"å®é™…ä½¿ç”¨çš„æ•°æ®æº: {response.source}")
            logger.info(f"è·å–åˆ° {len(response.data)} æ¡è®°å½•")
        else:
            logger.error(f"âŒ æ•…éšœè½¬ç§»å¤±è´¥: {response.error_message}")
        
        return response.success
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º4å¤±è´¥: {e}")
        return False


def demo_data_quality():
    """æ¼”ç¤ºæ•°æ®è´¨é‡è¯„ä¼°"""
    logger.info("\nğŸ” æ¼”ç¤º5: æ•°æ®è´¨é‡è¯„ä¼°")
    logger.info("=" * 60)
    
    try:
        data_service = get_data_service(enable_cache=True)
        
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-03-01",
            end_date="2024-03-07"
        )
        
        # ä½¿ç”¨è´¨é‡éªŒè¯
        response = data_service.get_data(
            request=request,
            merge_strategy='quality_based',
            use_cache=False
        )
        
        if response.success:
            logger.success("âœ… æ•°æ®è·å–æˆåŠŸ")
            logger.info(f"æ•°æ®æº: {response.source}")
            logger.info(f"è®°å½•æ•°: {len(response.data)}")
            
            # æ˜¾ç¤ºæ•°æ®è´¨é‡ä¿¡æ¯
            if hasattr(response, 'metadata') and response.metadata:
                metadata = response.metadata
                logger.info("\nğŸ“Š æ•°æ®è´¨é‡ä¿¡æ¯:")
                
                if 'source_count' in metadata:
                    logger.info(f"  å‚ä¸èåˆçš„æ•°æ®æºæ•°é‡: {metadata['source_count']}")
                
                if 'fusion_timestamp' in metadata:
                    logger.info(f"  èåˆæ—¶é—´: {metadata['fusion_timestamp']}")
            
            # åŸºæœ¬æ•°æ®è´¨é‡æ£€æŸ¥
            data = response.data
            if not data.empty:
                logger.info("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
                
                # æ£€æŸ¥ç¼ºå¤±å€¼
                missing_count = data.isnull().sum().sum()
                total_cells = data.size
                completeness = 1 - (missing_count / total_cells) if total_cells > 0 else 0
                logger.info(f"  æ•°æ®å®Œæ•´æ€§: {completeness:.2%}")
                
                # æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
                if 'close_price' in data.columns:
                    prices = data['close_price']
                    logger.info(f"  ä»·æ ¼èŒƒå›´: ${prices.min():.2f} - ${prices.max():.2f}")
                    
                    if (prices <= 0).any():
                        logger.warning("  âš ï¸ å‘ç°éæ­£ä»·æ ¼æ•°æ®")
                    else:
                        logger.info("  âœ… ä»·æ ¼æ•°æ®åˆç†")
                
                # æ£€æŸ¥æˆäº¤é‡
                if 'vol' in data.columns:
                    volumes = data['vol']
                    avg_volume = volumes.mean()
                    logger.info(f"  å¹³å‡æˆäº¤é‡: {avg_volume:,.0f}")
        else:
            logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {response.error_message}")
        
        return response.success
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º5å¤±è´¥: {e}")
        return False


def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    logger.info("\nâš¡ æ¼”ç¤º6: æ€§èƒ½å¯¹æ¯”")
    logger.info("=" * 60)
    
    try:
        data_service = get_data_service(enable_cache=True)
        
        # æ¸…ç©ºç¼“å­˜ä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
        data_service.clear_cache()
        
        request = DataRequest(
            data_type=DataType.DAILY_QUOTES,
            symbol="AAPL",
            start_date="2024-02-01",
            end_date="2024-02-29"
        )
        
        # æµ‹è¯•ä¸åŒç­–ç•¥çš„æ€§èƒ½
        strategies = [
            ('first_success', 'ç¬¬ä¸€ä¸ªæˆåŠŸ'),
            ('quality_based', 'åŸºäºè´¨é‡')
        ]
        
        performance_results = []
        
        for strategy, description in strategies:
            logger.info(f"\nğŸ“Š æµ‹è¯• {description} ç­–ç•¥æ€§èƒ½")
            
            start_time = time.time()
            response = data_service.get_data(
                request=request,
                merge_strategy=strategy,
                use_cache=False
            )
            elapsed_time = time.time() - start_time
            
            if response.success:
                performance_results.append({
                    'strategy': description,
                    'time': elapsed_time,
                    'records': len(response.data),
                    'source': response.source
                })
                
                logger.success(f"âœ… {description} å®Œæˆ")
                logger.info(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
                logger.info(f"è®°å½•æ•°: {len(response.data)}")
                logger.info(f"æ•°æ®æº: {response.source}")
        
        # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
        if len(performance_results) > 1:
            logger.info("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
            for result in performance_results:
                logger.info(f"  {result['strategy']}: {result['time']:.2f}ç§’ ({result['records']} è®°å½•)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤º6å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸŒŸ å¤šæ•°æ®æºä¸­é—´å±‚åŠŸèƒ½æ¼”ç¤º")
    logger.info("=" * 80)
    
    demos = [
        ("åŸºæœ¬æ•°æ®è·å–", demo_basic_usage),
        ("å¤šæ•°æ®æºèåˆ", demo_multiple_sources),
        ("ç¼“å­˜åŠŸèƒ½", demo_caching),
        ("æ•…éšœå®¹é”™", demo_fault_tolerance),
        ("æ•°æ®è´¨é‡è¯„ä¼°", demo_data_quality),
        ("æ€§èƒ½å¯¹æ¯”", demo_performance_comparison)
    ]
    
    success_count = 0
    total_count = len(demos)
    
    for demo_name, demo_func in demos:
        try:
            logger.info(f"\nğŸ¯ å¼€å§‹æ¼”ç¤º: {demo_name}")
            success = demo_func()
            if success:
                success_count += 1
                logger.success(f"âœ… {demo_name} æ¼”ç¤ºæˆåŠŸ")
            else:
                logger.error(f"âŒ {demo_name} æ¼”ç¤ºå¤±è´¥")
        except Exception as e:
            logger.error(f"âŒ {demo_name} æ¼”ç¤ºå¼‚å¸¸: {e}")
    
    # æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š æ¼”ç¤ºæ€»ç»“")
    logger.info("=" * 80)
    logger.info(f"æ€»æ¼”ç¤ºæ•°: {total_count}")
    logger.info(f"æˆåŠŸæ¼”ç¤º: {success_count}")
    logger.info(f"å¤±è´¥æ¼”ç¤º: {total_count - success_count}")
    logger.info(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    if success_count == total_count:
        logger.success("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºéƒ½æˆåŠŸå®Œæˆï¼")
        logger.info("\nğŸ’¡ å¤šæ•°æ®æºä¸­é—´å±‚çš„ä¸»è¦ä¼˜åŠ¿:")
        logger.info("  âœ… ç»Ÿä¸€çš„æ•°æ®æ¥å£ï¼Œç®€åŒ–å¼€å‘")
        logger.info("  âœ… å¤šæ•°æ®æºæ”¯æŒï¼Œæé«˜å¯é æ€§")
        logger.info("  âœ… æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œæå‡æ€§èƒ½")
        logger.info("  âœ… æ•…éšœè½¬ç§»èƒ½åŠ›ï¼Œä¿è¯å¯ç”¨æ€§")
        logger.info("  âœ… æ•°æ®è´¨é‡è¯„ä¼°ï¼Œç¡®ä¿å‡†ç¡®æ€§")
        logger.info("  âœ… çµæ´»çš„èåˆç­–ç•¥ï¼Œæ»¡è¶³ä¸åŒéœ€æ±‚")
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†æ¼”ç¤ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥")
    
    # æ¸…ç†èµ„æº
    try:
        from src.data_sources.data_source_factory import close_data_service
        close_data_service()
        logger.info("ğŸ”§ èµ„æºæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.warning(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    return success_count == total_count


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
